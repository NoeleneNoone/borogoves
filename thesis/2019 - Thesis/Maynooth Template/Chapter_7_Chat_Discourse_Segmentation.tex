% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Chat Discourse Segmentation and Boundary Identification} % top level followed by section, subsection
\label{ch:TM}

\begin{textsl}
{\small Collaborative chat tools and large text corpora are ubiquitous in today's world of real-time communication. As micro teams and start-ups adopt such tools, there is a need to understand the meaning (even at a high level) of chat conversations within collaborative teams. In this study, we propose a technique to segment chat conversations to increase the number of words available (19\% on average) for text mining purposes. Using an open source dataset, we answer the question of whether having more words available for text mining can produce more useful information to the end user. Our technique can help micro-teams and start-ups with limited resources to efficiently model their conversations to afford a higher degree of readability and comprehension.

% Hello & Goodbye Abstract
One of the main challenges in discourse analysis is the process of segmenting text into meaningful topic segments. While this problem has been studied over the past thirty years, previous topic segmentation studies ignore crucial elements of a conversation: an opening and closing remark. Our motivation to revisit this problem space is the rise of instant message usage. We consider the problem of topic segmentation as a machine learning classification one. Using both enterprise and open source datasets, we address the question as to whether a machine learning algorithm can be trained to identify salutations and valedictions within multi-party real-time chat conversations. Our results show that both Na\"ive Bayes and Support Vector Machine (SVM) algorithms provide a reasonable degree of precision(mean F1 score: 0.58).}
\end{textsl}

\vspace*{1cm}

%\adjustmtc

%\minitoc
% ----------------------- contents from here ------------------------


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%---------------------------------------- INTRODUCTION ----------------------------------------------------
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Introduction}
We live in an information age, and consumer-based services and applications generate more text-based data. As we embrace both collaborative and social communication, we converse more often via text-based communication \cite{talkmore2015} \cite{textstats2016}.  For both business and recreational purposes real-time chat discourse appears to be part and parcel of our lives. 

However, for businesses irrespective of size, using such collaborative and social means of communication, can be an overwhelming experience \cite{huffpost2016}. This is due in part to the large volumes of text-based data that are generated by such applications and services. Recent studies have shown that almost 350,000 tweets are created every minute of every day. Globally 2.5 quintillion bytes of data are produced \cite{venturebeat2016}. The growth in social media messaging is not confined to tweet messages. A recent study \cite{socmsg2017} by the Harvard business school has shown that over 2.5 billion users communicate with at least one messaging app (e.g. WhatsApp, Facebook). This figure will rise to 3.6 billion users in the next few years. Therefore, for this study, we consider techniques that may help teams make sense of their message based data.   

Topic modelling is a frequently used process to discover semantic structure within a text corpus. Topic modelling and text mining are used across multiple disciplines \cite{tmvalue2017} as a vehicle to grow business insights \cite{dti2017}. For example, if a business can mine customer feedback on a particular product or service this information may prove valuable \cite{softadvice2015}. One of the recommendations when employing text mining/topic modelling techniques is that the more data available for analysis, the better the overall results. However, even in the age of big data, practitioners may have a requirement to text mine a single conversation or small text corpus to infer meaning. 

In this paper, we propose a framework that both micro teams and SMEs can use to deliver a significant level of topic modelling terms, from real-time chat discourse data, while utilising their limited resource cohort. The core idea of this framework is for topic mining practitioners to partition their conversations using a novel technique. Such a method can provide a higher number of words (19\% on average) for topic summarisation tooling. For small teams with a limited pool of test resources, leveraging such segmentation techniques can provide not only more words for text mining but an improved level of readability than using an entire message corpus alone.

This paper contains research conducted on an open-source real-time chat corpus. Through the study of this dataset we investigate a) If by partitioning messages based on their inter-arrival time, can a more significant number of distinct words be returned for use by topic modelling software? b) Does a higher number of words provide a level of readability that is easier for humans to comprehend? c) Can we use the results of this work to predict an optimal topic cluster size? Using the results of this study for our framework, a topic mining solution can be developed to provide an enhanced level of understanding for small message corpora.

% Hello and Goodbye introduction

Real-time group chat applications are seen as a way to improve productivity within mobile teams \cite{mobilesix}. By using traditional conversational techniques, discussions can take place irrespective of location or time-zone.  Small to medium (SMEs) businesses cite multiple benefits of real-time chat rooms. Such advantages include, brainstorming, client conferencing, customer support and distance learning \cite{smallbizchat}. A recent survey by ReportLinker found that while email is a primary source of communication for communication, group messaging application use is on the rise \cite{replink}. 

However, there are a number of drawbacks to group chat applications. Often cited are the problems with continuous partial attention (i.e. routinely checking a conversation) \cite{procon} or the lack conversation summarisation \cite{chatsweat}. It is the latter problem that can prove challenging for users, especially if they have been away from a group chat for a period of time. Businesses face challenges in this regard, as current group chat applications offer little or no chat summarisation functionality. 

Text segmentation is a technique used to separate text into meaningful clusters. Such clusters may include sentences or topics. Previously, text segmentation research has focused on topic changes within written discourse. Such discourse comprises prose text \cite{likforman2007text}. However, in recent times attention has turned to conversational discourse such as real-time chat \cite{weisz2008segmentation}. In an age of big data, coupled with the fact that businesses are using collaboration applications more than before \cite{chatrise}, being able to segment chat conversations by topic may prove useful in the domain of information retrieval.

In this paper, we propose a technique that text segmentation practitioners can use to annotate conversations with an opening (salutation) and closing (valediction) remark. The core idea of this study is to demonstrate that by manually annotating conversation boundaries, a trained machine learning classifier algorithm can identify conversation boundaries using salutations and valedictions as a conversation perimeter. For topic modelling practitioners, identification of conversation boundary markers may improve text mining outcomes or a per-conversation basis.

This study contains research conducted on two real-time chat discourse datasets. Our first dataset is an enterprise dataset from a real-time collaboration application; our second dataset is an open source data set from an Internet chat relay (IRC) channel. We investigate a) the high frequency words and key collocations that are present in salutation \& valediction messages and b) whether two specific machine learning classification algorithms can identify salutations, valedictions and conversation body text within multi-party chat discourse. The results of our study can be used to further the body research in the field of text segmentation.


\section{Case study 7 - Chat discourse segmentation}

Topic modelling and text mining of social media/collaboration messaging have been shown to provide insight into the subjects people discuss as part of their online communication. By segmenting instant message text in a novel way, before topic modelling, we demonstrate how a higher degree of understanding can be achieved by the results of topic model outputs. 

For this study, we topic modelled three complete chat conservations from an open source Ubuntu developer IRC channel \cite{irclogs}. For each conversation, IRC messages were read, we noted an initial salutation, a valediction and a grouped topic discussed in-between the greeting and farewell messages. For this study, only conversations with related topic content were considered. We note that chat conversations with mixed chat messages (i.e. `entangled chat conversations') are beyond the scope of this study and will not be considered here. 

\begin {table}[]
\caption {Summary of Dataset Conversation Metrics} 
\label{tab:chapt7_study1_tab1}
\begin{center}
\begin{tabular}{| p{5.5cm} | p{1.5cm} | p{1.5cm} | p{1.5cm} |} \hline
\textbf{Metric} & \textbf{Chat 1} & \textbf{Chat 2} & \textbf{Chat 3} 
\\ \hline Total Messages & \multicolumn{0}{|r|}{46} & \multicolumn{0}{|r|}{70} & \multicolumn{0}{|r|}{59}
\\ \hline Total Words & \multicolumn{0}{|r|}{292} & \multicolumn{0}{|r|}{436} & \multicolumn{0}{|r|}{484}
\\ \hline Non-Stopped Words & \multicolumn{0}{|r|}{158} & \multicolumn{0}{|r|}{239} & \multicolumn{0}{|r|}{262}
\\ \hline Distinct Non-Stopped words & \multicolumn{0}{|r|}{111} & \multicolumn{0}{|r|}{168} & \multicolumn{0}{|r|}{186}
\\ \hline \% Words for analysis & \multicolumn{0}{|r|}{38} & \multicolumn{0}{|r|}{39} & \multicolumn{0}{|r|}{38} 
\\ \hline
\end{tabular}
\end{center}
\end{table}


Table~\ref{tab:chapt7_study1_tab1} provides a summary of the number of total words, the non-stopped words, the distinct non-stopped words and the percentage of words available for analysis. 

This study aims to answer the following questions. First, can we segment a chat conversation in such a way as to provide a greater number of distinct words for topic modelling algorithms? Second, if a reasonable segmentation method can be found, is the output from a topic model easier to infer meaning, then modelling the entire conversation alone? Third, is there a relationship between the topic modelling cluster size and the number of words Input/Output from topic modelling? 

\subsection{Conversation segmentation}

A question for practitioners of topic modelling is, how can we maximise the number of words for analysis? We know from prior research that text mining algorithms that some form of text pre-processing is required prior to topic modelling. Pre-processing may include at least one of the following: Tokenisation, stop words removal, stemming and lemmatisation. The removal of words as part of this pre-processing step usually is not an issue for a large text corpus, due to the number of words available. In the case of small text corpora, the problem may be more acute. For our study, stemming and lemmatisation was not conducted.

We recorded the inter-arrival time of instant message posts within the Ubuntu IRC channel, and grouped messages by short and long inter-arrival times. For successive messages with a zero minute inter-arrival time, we define this collection of messages as a burst. For messages with a one minute or greater inter-arrival time, we define this group of messages as a reflection. We then perform text mining on each burst and reflection period and then aggregate the output terms. For topic text mining, we used the tool Biterm, which is suited to modelling small text corpora.

\subsection{Topic modelling comprehension}

\begin {table}[]
\caption {Summary of Differences between Questionnaire Samples} 
\label{tab:chapt7_study1_tab1a}
\begin{center}
\begin{tabular}{| p{2.5cm} | p{6.9cm}  |} \hline 
\textbf{Sample 1} & Entire chat - Topic Modelled  
\\ \hline  \textbf{Sample 2} & Burst \& Reflections - Topic Modelled
\\ \hline  \textbf{Sample 3} & Entire chat - Stop words removed 
\\ \hline  \textbf{Sample 4} & Entire chat - No text pre-processing 
\\ \hline
\end{tabular}
\end{center}
\end{table}

After a conversation has been a) segmented into burst and reflection periods, b) these periods topic modelled and c) the results aggregated, we consider the efficacy of the output. 

We accept that the terms output from a topic model algorithm is not explicitly designed for a readable summary. Instead, they are designed to give a user an indication of the terms used in a text corpus. Nevertheless of interest is how a user can understand the output of text mining. Our approach is to prepare four sets of text as follows; 1) each conversation is modelled with Biterm (as a whole) and the mined terms output into a single collection, 2) the bursts and reflections from each conversation are modelled individually, the terms are then aggregated into a single collection, 3) each conversation with the stop words removed and 4) the raw conversation (i.e. without any pre-processing). Table ~\ref{tab:chapt7_study1_tab1a} summarises the level of pre-processing conducted for each sample.

We then asked twenty four test subjects to summarise each of the four text sets belonging to a single conversation. Additionally, we asked each participant to comment on which of the four text sets was easiest to summarise. Next, we asked each subject, whether they felt set one (all terms topic modelled) or set two (bursts and reflections topic modelled) was most natural to summarise. Finally, we asked each subject to describe why they felt the text set chosen in the second question was easiest to summarise. Results of a meta-study on sample sizes for qualitative studies \cite{qualsample} show there is variability in sample size depending on the subject domain. For our questionnaire, twenty-four individuals were selected, and each conversation was randomly distributed among the users.

Finally we compared the readability of every text set for each conversation using a number of known readability tests such as; Dale-Chall \cite{dale1948formula}, Coleman-Liau \cite{coleman1975computer}, Flesch-Kincaid \cite{kincaid1975derivation} and Gunning Fog \cite{gunning1952technique}. 

\subsection{Term cluster size prediction}

Topic modelling algorithms use a unique set of words from a corpus for analysis. Also, we know that the process of text mining may be, in part non-deterministic. In other words, random sampling is often used to generate a term list. One of the goals of text mining is to ensure that a sufficient number of words are output in each topic cluster. The intuition is that the more unique words that are provided, perhaps the easier the output will be to understand.

Biterm, outputs topic mined terms as `topic clusters'. Each cluster has a maximum size of ten terms. If one hundred words are input for analysis, the intuition is that ten clusters will be output with a ten distinct words. However, due to the underlying random nature of the sampling algorithm used, this is not always the case. Therefore, it is necessary to use a range of cluster sizes to obtain the optimal number of terms. We define `optimal output words' as the number of words that closely matches the number of words used for Biterm analysis. We define the `optimal \# clusters', as the smallest number of clusters that contains the optimal output words.

Linear regression is a method to determine the relationship between two or more variables where one variable is dependent, and the additional variables are independent. A hypothesis test (are two or variables correlated) is conducted, and a p-value is computed. Depending on the size of the p-value, the hypothesis of a relationship/no relationship can be accepted or rejected. We used the lm function found in the base R package \cite{lmR} and performed a linear regression test.

We will use linear regression to explore the relationship between the number of unique words input, the Biterm cluster size and the unique terms output. For example, if we model the unique words input to Biterm, the cluster size that provides the unique optimal set of terms and the count of these text mined terms, a linear model could be used to predict optimal term cluster size.

\subsection{Limitations of dataset}

The dataset has some practical limitations, which are now discussed. The process of aggregating chat messages into a cohesive conversation is a subjective one. Every effort was made to assign messages to their most appropriate thread. We recognise that the process is subjective. Additionally, the post times for the Ubuntu chat were measured in hours and minutes only. As a result, we defined our burst and reflection period as timed in zero and one minute or greater priors respectively. 

The chat conversations that form part of this case study are from an Ubuntu IRC developer channel. While we hope these examples will be representative of technical discussion channels, it seems unlikely they will be typical of all types of channels.

\section{Results}

We now explore the results of our analysis.

\subsection{Conversation segmentation}

\begin {table}[]
\caption {Summary of Text Mining Analysis: Entire Conversations Vs Burst and Reflections} 
\label{tab:chapt7_study1_tab2}
\begin{center}
\begin{tabular}{| p{6.8cm} | p{1.5cm} | p{1.5cm} | p{1.5cm} |} \hline \textbf{Metric} & \textbf{Chat 1} & \textbf{Chat 2} & \textbf{Chat 3} 
\\ \hline Total words & \multicolumn{0}{|r|}{292} & \multicolumn{0}{|r|}{436} & \multicolumn{0}{|r|}{484} 
\\ \hline Non-stopped words & \multicolumn{0}{|r|}{158} & \multicolumn{0}{|r|}{239} & \multicolumn{0}{|r|}{262} 
\\ \hline Distinct non-stopped words & \multicolumn{0}{|r|}{111} & \multicolumn{0}{|r|}{168} & \multicolumn{0}{|r|}{186}
\\ \hline Distinct non-stopped terms output & \multicolumn{0}{|r|}{96} & \multicolumn{0}{|r|}{129} & \multicolumn{0}{|r|}{143} 
\\ \hline \# Words not analysed & \multicolumn{0}{|r|}{196} & \multicolumn{0}{|r|}{307} & \multicolumn{0}{|r|}{341} 
\\ \hline \% Words for analysis & \multicolumn{0}{|r|}{38} & \multicolumn{0}{|r|}{39} & \multicolumn{0}{|r|}{38} 
\\ \hline \% Actual terms output & \multicolumn{0}{|r|}{33} & \multicolumn{0}{|r|}{30} & \multicolumn{0}{|r|}{30} 
\\ \hline & & & 
\\ \hline Total burst words & \multicolumn{0}{|r|}{185} & \multicolumn{0}{|r|}{226} & \multicolumn{0}{|r|}{287} 
\\ \hline Non-stopped burst words & \multicolumn{0}{|r|}{98} & \multicolumn{0}{|r|}{143} & \multicolumn{0}{|r|}{163} 
\\ \hline Distinct non-stopped burst words & \multicolumn{0}{|r|}{91} & \multicolumn{0}{|r|}{118} & \multicolumn{0}{|r|}{154} 
\\ \hline Distinct non-stopped terms output & \multicolumn{0}{|r|}{87} & \multicolumn{0}{|r|}{118} & \multicolumn{0}{|r|}{145} 
\\ \hline \# Burst words not analysed & \multicolumn{0}{|r|}{94} & \multicolumn{0}{|r|}{108} & \multicolumn{0}{|r|}{142} 
\\ \hline \# Bursts & \multicolumn{0}{|r|}{7} & \multicolumn{0}{|r|}{11} & \multicolumn{0}{|r|}{8} 
\\ \hline \% Words for analysis & \multicolumn{0}{|r|}{49} & \multicolumn{0}{|r|}{52} & \multicolumn{0}{|r|}{54} 
\\ \hline \% Actual terms output & \multicolumn{0}{|r|}{47} & \multicolumn{0}{|r|}{52} & \multicolumn{0}{|r|}{51} 
\\ \hline & & & 
\\ \hline Total Reflection words & \multicolumn{0}{|r|}{107} & \multicolumn{0}{|r|}{210} & \multicolumn{0}{|r|}{197} 
\\ \hline Non-stopped reflection words & \multicolumn{0}{|r|}{61} & \multicolumn{0}{|r|}{99} & \multicolumn{0}{|r|}{99} 
\\ \hline Distinct non-stopped reflection words & \multicolumn{0}{|r|}{60} & \multicolumn{0}{|r|}{95} & \multicolumn{0}{|r|}{94} 
\\ \hline Distinct non-stopped terms output & \multicolumn{0}{|r|}{60} & \multicolumn{0}{|r|}{93} & \multicolumn{0}{|r|}{94} 
\\ \hline \# Reflection words not analysed & \multicolumn{0}{|r|}{47} & \multicolumn{0}{|r|}{115} & \multicolumn{0}{|r|}{103} 
\\ \hline \# Reflections & \multicolumn{0}{|r|}{7} & \multicolumn{0}{|r|}{12} & \multicolumn{0}{|r|}{9} 
\\ \hline \% Words for analysis & \multicolumn{0}{|r|}{56} & \multicolumn{0}{|r|}{45} & \multicolumn{0}{|r|}{48} 
\\ \hline \% Actual terms output & \multicolumn{0}{|r|}{56} & \multicolumn{0}{|r|}{44} & \multicolumn{0}{|r|}{48} 
\\ \hline
\end{tabular}
\end{center}
\end{table}


Table \ref{tab:chapt7_study1_tab2} shows a summary of the topic modelling conducted on each of the three conversations. In the first experiment, the entire discussion was mined. In the second experiment, the burst and reflections were modelled separately. 

For conversation one, a total of 96 terms were output by Biterm when modelling the entire text, whereas 87 and 60 terms respectively were output from the burst and reflection analysis. A total of 51 (17\%) more terms were output from the combined burst and reflection analysis than modelling the entire conversation.

For conversation two, a total of 129 terms were output by Biterm, whereas 118 and 93 terms respectively were output from the burst and reflection analysis. A total of 82 (19\%) more terms were output from the combined burst and reflection analysis than modelling the entire conversation.

For conversation three, a total of 143 terms were output by Biterm, whereas 145 and 94 terms respectively were output from the burst and reflection topic mining. A total of 96 (20\%) more terms were output from the combined burst and reflection analysis than modelling the entire conversation.

\subsection{Topic modelling comprehension}

\begin {table}[]
\caption {Summary of Text Sample Questionnaire Answers (Q1 \& Q2)} 
\label{tab:chapt7_study1_tab3}
\begin{center}
\begin{tabular}{| p{3.5cm} | p{2cm} | p{2cm} | p{2cm} | p{2cm} |} \hline \textbf{Question} & \textbf{Sample 1 - Biterm (All text)} & \textbf{Sample 2 - Biterm (Burst \& Reflections)} & \textbf{Sample 3 - (Stop words removed)} & \textbf{Sample 4 - (Full text)} 
\\ \hline One: Of the 4 text samples, which sample did you find easier to summarise? (1/2/3 or 4) & 0 & 0 & 2 & 22
\\ \hline Two:  Of samples 1 and 2, which sample did you find easier to summarise? (1 or 2) & 0 & 24 & NA & NA
\\ \hline
\end{tabular}
\end{center}
\end{table}

Recalling the survey questions asked in section III part B: Of the four text samples, which sample did you find easier to summarise? And of samples 1 and 2, which sample did you find easier to summarise?
Table \ref{tab:chapt7_study1_tab3} shows a summary of the answers to the questions asked of the test subjects. Before the questionnaire, the subjects were asked to summarise the four samples. The questions were asked directly after the summarisation task. As we can see for question one, the majority of users found sample 4 easiest to summarise. For question two, the respondents answered unanimously in favour of sample 2.

\begin{figure}
\begin{center}
\includegraphics[height=9cm, width=14cm]{graphs/bundles/wordcloud_v2.pdf} 
\caption{Word Cloud of answers from survey question 3}
\end{center}
\label{fig:chapt7_study1_fig1}
\end{figure}

Question three asked: For the sample, you chose in question two, why did you find that text sample easier to summarise? Figure 7.1 shows a word cloud generated from the answers respondents provided. When stop words were removed, the following terms appeared most frequently: easier (8 times), text (6), words (5) and flow/natural/understand/words (all 5).

\begin{figure}
\begin{center}
\includegraphics[height=9cm, width=14cm]{graphs/bundles/readindex1.pdf} 
\caption{Mean readability index score of each text sample}
\end{center}
\label{fig:chapt7_study1_fig2}
\end{figure}

To further understand the readability of text output from our topic mining experiments, we conducted some readability tests (Dale-Chall, Coleman-Liau, Flesch-Kincaid and Gunning Fog) against each of the four text samples for all three conversations. Figure 7.2 shows a bar chart of mean readability index scores. In all cases, a lower score indicates a more readable text sample. Intuitively we can see that text sample 1 had the highest score across all indices, and text sample 4 had a lowest. Text sample 2 had a lower index score than sample 1 in all readability tests.

% Thesis candidate only
\subsection{Term cluster size prediction}

Our third research question asked, ``Can we use the results of our topic modelling to predict an optimal topic bundle size?'' We mentioned previously that obtaining an optimal number of terms (i.e. an output number of distinct words that matches an input number of distinct words) from Biterm is an iterative process. 

For each burst, reflection and complete segment we topic modelled multiple cluster numbers to obtain the optimal number of distinct words. Once an optimal cluster size was found, the number of clusters was noted. We then used Linear regression to determine if there is a strong relationship between the number of distinct words output and the cluster size. In order words can we create a linear function to predict the number of topic clusters, if the optimal number of terms are known?


\begin {table}[]
\caption {Linear Regression Coefficient Table} 
\label{tab:chapt7_study1_tab4}
\begin{center}
\begin{tabular}{| p{3cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} |} \hline \textbf{Coefficients} & \textbf{Estimate} & \textbf{Std. Error} & \textbf{t value} & \textbf{Pr(\textgreater t)} 
\\ \hline (Intercept) & 0.934 & 0.122 & 7.606 & 3.45e-10
\\ \hline Optimal.Terms & 0.058 & 0.003 & 18.201 & \textless 2e-16
\\ \hline
\end{tabular}
\end{center}
\end{table}

Table \ref{tab:chapt7_study1_tab4} shows the output of the Linear regression analysis. We also note the following additional outputs;  Residual standard error: 0.818, Multiple R-squared:  0.855,	Adjusted R-squared:  0.853 and p-value: \textless 2.2e-16. From the output we can see that the equation to fit our linear model is as follows:

\begin{equation}
Number of Clusters = 0.934 + 0.058(Optimal.Terms)
\end{equation}

\begin{figure*}[]
\begin{center}
\includegraphics[height=9cm, width=14cm]{graphs/bundles/linear_fit1.pdf} 
\caption{Residuals Vs Fitted, Q-Q Plot, Scale-Location \& Residuals Vs Leverage}
\end{center}
\label{fig:chapt7_study1_fig3}
\end{figure*}

Figure 7.3 shows the four goodness-of-fit plots generated from our linear regression model. These plots are used in conjunction with the results of the Linear coefficients table to determine the suitability of the model. We shall discuss these results of the model in more detail in the next section.

\section{Discussion}

The following section provides deeper analysis and discussion of the results. In each section, references will be made to each research question asked in section 7.2.

\subsection{Conversation segmentation}

Our first research question asked, can we segment a chat conversation in such a way as to provide a higher number of unique words for topic modelling algorithms? Table \ref{tab:chapt7_study1_tab2} shows that the mean proportion of words available for analysis for topic modelling of an entire conversation is 38\%, this is due to the considerable number of stop words that are removed as part of pre-processing. Likewise, the mean number of terms output from Biterm is 31\% a reduction of 7\%.

Conversely, when both burst and reflections are aggregated, the mean proportion of terms available for analysis is 51\%. Furthermore, the mean proportion of terms output from Biterm is on average, 50\% a reduction of only 1\%.

There is evidence to suggest that segmenting conversations into shorter segments provides a greater number of words for topic analysis due to the lack of duplicate words found in each smaller segment. We note that stop words are removed irrespective of the segment size.

Some interesting points are raised by our analysis. When stop words are removed, duplicate occurrences of the same word are also discarded. However, in the case of a more substantial text corpus, some duplicate non-stop words remain, these words are ignored by text mining tools. We see this is not the case with burst and reflection text segments. In fact, for conversation 2, 143 non-stopped words were retained. A further 25 non-stop duplicate words were ignored. In all other cases, the number of duplicate words ignored by Biterm after stop words were removed was less than 10.

Furthermore, we observed that the number of burst and reflections created might have little significance on the number of terms output. Conversations 1 \& 3 had a similar number of segments (i.e. between 7 and 9), while conversation 2 had 11 burst and 12 reflections respectively. While no formal correlation tests were conducted, when we look at the segment size and the \% terms output, there seems little positive or negative relationship between the two variables.


\subsection{Topic modelling comprehension}

Our second research question asked: If a reasonable segmentation method can be found, is the output from a topic model easier to understand, than modelling the entire conversation alone? In other words, even if more words can be output as part of our improved segmentation technique, how does this translate into comprehension by both a human and for general readability. 

Table \ref{tab:chapt7_study1_tab3} summarises the answers to the first two survey questions asked by the 24 individuals who took part in our topic modelling comprehension experiment. Unsurprisingly we can see that the majority of respondents picked sample 4, as the easiest to summarise. The consensus was that with all words available and with grammar respected (to a degree) sample 4 was easiest to summarise for the majority. However, we note that two respondents picked sample 3 (stop words removed). The feedback from these two participants was that the samples with fewer words were easier to understand, this may be because these two individuals were not Ubuntu experts.

For survey question 2 the unanimous feedback from all users was that sample 2 was much easier to read than sample 1. A word cloud produced from the short answers provided, clearly indicate that a combination of our segmentation technique and Biterm preserved the natural flow of the conversation to the degree that it was easier to summarise the text sample than sample 1.

Turning to the readability tests conducted, we can see that sample 4 produced the lowest mean index score indicating that the unprocessed chat was the most readable based on the four tests conducted. Except for the Gunning Fog index, sample 2 had equal or lower readability scores than sample 3. 

It would be over-simplistic to state that when more words are available, it is easier for a human to understand a text segment based on a list of topic terms. However, it seems reasonable to assert that when more words are available and when words are placed in a similar order as to how they were typed, it is easier for humans to comprehend. What was interesting to note for short burst and reflection segments, (i.e. ten words or less) the input order of words was the same as the output terms produced by Biterm. That is to say the word at the start of the sentence had the highest log-likelihood value, while the word at the end of the sentence had the lowest log-likelihood value.

We note that the goal of this research question was not to provide a readable summary based on text mined terms. Instead, the goal was to assess the understanding of text samples to humans when varying degrees of topic mining is conducted. 

% Thesis candidate only
\subsection{Term cluster size prediction}

Our third research question asked: Can we use the results of our topic modelling experiments to predict the optimal cluster size? Previously, we discussed the problem of determining the number of clusters that will return the highest number of distinct words from the Biterm analysis. We also mentioned that the optimal cluster number could be obtained only by iteratively trying a range of sizes.

Table \ref{tab:chapt7_study1_tab4} provides the output of a linear regression experiment whereby we used the optimal terms to predict cluster size. The first point to note is that the p-value for optimal.terms was \textless2e-16, this figure indicts a strong regression effect. Additionally, we note that the multiple R-squared and adjusted R-squared values were 0.855 and 0.853. These values indicate the model is an excellent fit for our data.

Figure 7.3 depicts four goodness-of-fit plots to assess the goodness of fit of our model graphically. The residuals Vs fitted plot shows our model passes through the majority of fitted values quite well. It appears that a small number of points are outside the fitted line. The normal Q-Q plot shows the standardised residuals fitted against a normal distribution line. For the most part, almost all values fit the line. There are a number of exceptions, such as a few small and large residuals. This plot indicates the model would almost all values however there would be some uncertainty of fitted very small and very large values. Finally, three observations had leverage greater than 0.1.  

We mentioned previously that Biterm topic clusters contain ten terms, and that due to random variation of the tooling, it is not always possible to obtain the same level of distinct output terms as were input. For example, dividing the number of distinct words for analysis by ten and using this result as the optimal \# of clusters, does not provide the same amount of output words. However, we did not know to what degree of fit a linear model would provide. In our case, a good fit was obtained.

The main benefit such a linear model is as follows: Determining the optimal cluster size can be a time-consuming task, especially for large datasets. Even using a rule of thumb such as a `divide input distinct words by ten' as a starting point, multiple iterations of Biterm may be required. By using a linear model such as ours, the task of determining the optimal term cluster size may be expedited. 

\section{Case study 8 - Conversation Boundary Identification}

Text segmentation of social media/collaboration messaging can be a useful technique to improve the quality of text mining and summarisation tasks. By annotating conversation boundaries by their salutation and valedictions, we demonstrate how machine learning algorithms can be trained to identify such opening and closing remarks with a reasonable degree of precision.

\begin {table}[]
\caption {Summary of dataset metrics and boundaries} 
\label{tab:chap7_study2_tab5}
\begin{center}
\begin{tabular}{| p{6.2cm} | p{2.2cm} | p{3.5cm} |} 
\hline \textbf{Dataset} & \textbf{Enterprise} & \textbf{Ubuntudev-IRC}
\\ \hline \bf{Total \# Messages} & \multicolumn{0}{|c|}{3261} & \multicolumn{0}{|c|}{4223}
\\ \hline \bf{ Total dataset duration (hours)} & \multicolumn{0}{|c|}{4822} & \multicolumn{0}{|c|}{86}
\\ \hline \bf{Multi-line conversations annotated} & \multicolumn{0}{|c|}{257} & \multicolumn{0}{|c|}{207}
\\ \hline \bf{Salutations} & \multicolumn{0}{|c|}{257} & \multicolumn{0}{|c|}{207}
\\ \hline \bf{Valedictions} & \multicolumn{0}{|c|}{257} & \multicolumn{0}{|c|}{207}
\\ \hline
\end{tabular}
\end{center}
\end{table}

The study presented in this paper examines four hundred and sixty-four manually annotated real-time chat conversations from two datasets. The details are summarised in Table~\ref{tab:chap7_study2_tab5}. 

For each message, we noted whether it was a salutation (i.e. an opening remark, greeting etc.). Each subsequent message was read until a valediction (i.e. a closing remark, farewell or acknowledgement message) was found. With a conversation perimeter identified we assigned a numeric topic ID. A number of single-line topics were found as part of the annotation process. For this study, only multi-line topics (i.e. conversations with one distinct salutation and valediction) are considered as part of our analysis.   

The first dataset analysed was from an enterprise instant message chat system which discussed cloud infrastructure problems. For our study, we reviewed approximately 3200 messages. As part of the review phase, we annotated 257 distinct conversations. The total time period analysed was approximately 4820 hours. 

The second dataset\footnote{A copy of the annotated open-source dataset is available from the corresponding author upon request.}analysed was the open source Ubuntu dev IRC channel \cite{irclogs}. For our study, we reviewed approximately 4200 messages. As part of the review phase, we annotated 207 unique conversations. The total time period analysed was approximately 86 hours. 

This study aims to answer the following questions. First, what types of words and collations are contained within salutation and valediction messages? Second, can a classifier algorithm be trained to identify salutary and valedictory text from real-time chat messages, thus identify a conversation boundary?

\subsection{Lexicography}

Lexicography is the study of vocabulary meaning and its use. In recent times, research has expanded to include a corpus based approach \cite{biber1998corpus}. The benefit of a corpus based approach is as follows:

\begin{itemize}
  \item What is the frequency of word usage?
  \item What is the frequency of word usage across multiple senses?
  \item Do words have a systematic association with other words?
\end{itemize}

The advantage of the corpus-linguistic method is that language researchers can analyse naturally occurring language text produced by a variety of authors to confirm or refute intuitions about specific language features using empirical data.

We aggregated both the salutation and valediction messages from each dataset into a single corpus. We then used the corpus linguistic tool \#lancsbox \cite{lancsbox} to analyse our salutation and valediction words. Our first research question asked a) what are the high-frequency words used and b) what interesting collations are present in salutation \& valediction messages.

\subsection{Chat boundary classification}

\begin {table}[]
\caption {Condensed IRC conversation with classification labels} 
\label{tab:chap7_study2_tab6}
\begin{center}
\begin{tabular}{| p{3.2cm} | p{2.5cm} | p{3cm} | p{2cm} |} \hline \textbf{Date} & \textbf{User} & \textbf{Test} & \textbf{Label} 
\\  \hline 01/10/04 06:20 & \textless m\_tthew$>$ & fabbione: ahoy & salutation
\\  \hline 01/10/04 06:27 & \textless fabbione$>$ & hey m\_tthew & undefined
\\ \hline 01/10/04 07:04 & \textless mdz$>$ & morning & undefined
\\  \hline 01/10/04 07:11 & \textless fabbione$>$ & mdz: for the ati / flrdkjdjds driver... & undefined
\\  \hline 01/10/04 07:18 & \textless fabbione$>$ & building now :-) & undefined 
\\  \hline 01/10/04 07:18 & \textless fabbione$>$ & brb & valediction 
\\ \hline
\end{tabular}
\end{center}
\end{table}

Our second research question asked, can a machine learning classifier algorithm be trained to identify text as a salutary and valedictory text from real-time chat messages with a reasonable degree of precision. For this research question, we constructed a multinomial classification experiment using three classes; salutation (opening message), undefined (neither an opening or closing message) and valediction (closing message). Table~\ref{tab:chap7_study2_tab6} provides an overview of the classification methodology.

There is an open question as to whether stop words (i.e. the most common words in a language) should be removed before classification. If stopwords are removed, it can remove ``noise" from sentences and allow a classifier to focus on a subset of text. However, the concern is that valuable text markers may be lost if such text is removed. We conduct our classification experiment on both the full text and with stop words removed.

Next, we conducted the following steps to train each algorithm and evaluate each training set using the python library scikit-learn\footnote{http://scikit-learn.org/stable/}.

\begin{itemize}
  \item Each dataset was divided into a training, development and test set, in a ratio of 60\% / 20\% / 20\%.
  \item Each training set was trained against both classifier algorithms.
  \item A development set was used to assess the performance of each algorithm.
  \item A test set was used to assess the performance of each algorithm to assess over/under-fitting.
  \item The above steps were repeated with the stop words removed from both datasets.
\end{itemize}

Note: For the SVM algorithm we assessed a total of thirty-six combinations of loss and penalty functions using the development set. The highest performing combination was then used to validate the training set, using the same parameters. The NB algorithm has no loss or penalty tuning parameters.

\subsection{Limitations of dataset}

The dataset has some practical limitations, which are now discussed. The process of aggregating chat messages into a cohesive conversation is a subjective one. Every effort was made to assign messages to their most appropriate thread. We recognise that the process is subjective, and may be subject to type I errors. 

The chat conversations that form part of this study are from an Ubuntu IRC developer channel and an Enterprise Cloud channel. While we hope these examples will be representative of technical discussion channels, it seems unlikely they will be typical of all types of channels.



\subsection{Results - Lexicography}

\begin {table}[]
\caption {List of the most frequent non stop words} 
\label{tab:chap7_study2_tab7}
\begin{center}
\begin{tabular}{| p{1cm} | p{2cm} | p{2cm} | p{1cm} | p{2cm} | p{2cm} |} 
\hline \textbf{\#} & \textbf{Word} & \textbf{Frequency} & \textbf{\#} & \textbf{Word} & \textbf{Frequency}
\\  \hline 17 & thanks &\multicolumn{0}{|c|}{68} & 35 & jenkins &\multicolumn{0}{|c|}{36}
\\  \hline 20 & can & \multicolumn{0}{|c|}{64} & 36 & ok &\multicolumn{0}{|c|}{36}
\\ \hline 21 & now & \multicolumn{0}{|c|}{51} & 37 & update &\multicolumn{0}{|c|}{35}
\\  \hline 24 & will & \multicolumn{0}{|c|}{49} & 40 & get &\multicolumn{0}{|c|}{35}
\\  \hline 27 & today & \multicolumn{0}{|c|}{42} & 42 & please &\multicolumn{0}{|c|}{33}
\\  \hline 29 & not & \multicolumn{0}{|c|}{40} & 43 & need &\multicolumn{0}{|c|}{33}
\\  \hline 30 & as & \multicolumn{0}{|c|}{40} & 44 & morning &\multicolumn{0}{|c|}{33}
\\  \hline 31 & new & \multicolumn{0}{|c|}{38} & 45 & working &\multicolumn{0}{|c|}{31}
\\  \hline 34 & just & \multicolumn{0}{|c|}{36} & 48 & still &\multicolumn{0}{|c|}{29}
\\ \hline
\end{tabular}
\end{center}
\end{table}

We tabulated a list of the fifty most common words found in both salutation and valediction text. Thirty-two of these words were found to be either stop words or usernames. We removed these words from our list. Table~\ref{tab:chap7_study2_tab7} provides a summary of the most common words used across the salutation and valediction corpus. We include the absolute rank of the word frequency in the overall corpus. 

\begin{figure}[]
\begin{center}
\includegraphics[height=7cm, width=9cm]{graphs/hellogoodbye/morning.pdf} 
\caption{Collation plot for the word `morning'}
\end{center}
\label{fig:chapt7_study2_fig1}
\end{figure}

\begin{figure}[]
\begin{center}
\includegraphics[height=7cm, width=9cm, angle =270]{graphs/hellogoodbye/thanks.pdf} 
\caption{Collation plot for the word `thanks'}
\end{center}
\label{fig:chapt7_study2_fig2}
\end{figure}

We choose two words: \textit{morning} and \textit{thanks} from Table~\ref{tab:chap7_study2_tab7} to explore collations in more detail. We selected \textit{thanks} as we observed that it was used quite frequently in a number of closing messages to acknowledge completion of some task. We selected \textit{morning} as it appeared in a number of salutations as a form of greeting. Fig.1 \& Fig. 2 illustrate a collocation graph for each word. A collocation graph is used to show how a given word is used in conjunction with other words, and whether that word appears to the left or right of a collocated word.

\subsection{Results - Chat boundary classification}

\begin {table*}[]
\caption {Summary of Text Classification Experiments} 
\label{tab:chapt7_study2_tab8}
\begin{center}
\begin{tabular}{| p{1.7cm} | p{2.2cm} | p{1.8cm} | p{1.8cm} | p{1.7cm} | p{1.3cm} | p{0.8cm} |}
 \hline \textbf{Dataset} & \textbf{Processing} & \textbf{Classifier} & \textbf{Label} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} 
\\ \hline Ubuntu IRC & None & NB & salutation & \multicolumn{0}{|r|}{0.52} & \multicolumn{0}{|r|}{0.55} & \multicolumn{0}{|r|}{0.54} 
\\ \hline  &  &  & undefined & \multicolumn{0}{|r|}{0.57} & \multicolumn{0}{|r|}{0.8} & \multicolumn{0}{|r|}{0.67} 
\\ \hline  &  &  & valediction & \multicolumn{0}{|r|}{0.55} & \multicolumn{0}{|r|}{0.3} & \multicolumn{0}{|r|}{0.39} 
\\ \hline  &  &  & \bf{mean} & \multicolumn{0}{|r|}{\bf{0.55}} & \multicolumn{0}{|r|}{\bf{0.55}} & \multicolumn{0}{|r|}{\bf{0.53}}

\\ \hline Ubuntu IRC & Tokenised \& & SVM & salutation  & \multicolumn{0}{|r|}{0.62} & \multicolumn{0}{|r|}{0.45} & \multicolumn{0}{|r|}{0.52} 
\\ \hline  &  Stopwords removed &  & undefined & \multicolumn{0}{|r|}{0.54} & \multicolumn{0}{|r|}{0.7} & \multicolumn{0}{|r|}{0.61} 
\\ \hline  &  &  & valediction & \multicolumn{0}{|r|}{0.62} & \multicolumn{0}{|r|}{0.6} & \multicolumn{0}{|r|}{0.61}   
\\ \hline  &  &  & \bf{mean} & \multicolumn{0}{|r|}{\bf{0.59}} & \multicolumn{0}{|r|}{\bf{0.58}} & \multicolumn{0}{|r|}{\bf{0.58}}  

\\ \hline Enterprise & None & SVM & salutation & \multicolumn{0}{|r|}{0.55} & \multicolumn{0}{|r|}{0.62} & \multicolumn{0}{|r|}{0.58}
\\ \hline  &  &  & undefined & \multicolumn{0}{|r|}{0.57} & \multicolumn{0}{|r|}{0.58} & \multicolumn{0}{|r|}{0.57}  
\\ \hline  &  &  & valediction & \multicolumn{0}{|r|}{0.56} & \multicolumn{0}{|r|}{0.48} & \multicolumn{0}{|r|}{0.52}  
\\ \hline  &  &  &\bf{mean} & \multicolumn{0}{|r|}{\bf{0.56}} & \multicolumn{0}{|r|}{\bf{0.56}} & \multicolumn{0}{|r|}{\bf{0.56}}  

\\ \hline Enterprise & Tokenised \& & SVM & salutation  & \multicolumn{0}{|r|}{0.68} & \multicolumn{0}{|r|}{0.64} & \multicolumn{0}{|r|}{0.66} 
\\ \hline  &  Stopwords removed &  & undefined & \multicolumn{0}{|r|}{0.49} & \multicolumn{0}{|r|}{0.58} & \multicolumn{0}{|r|}{0.53} 
\\ \hline  &  &  & valediction & \multicolumn{0}{|r|}{0.57} & \multicolumn{0}{|r|}{0.5} & \multicolumn{0}{|r|}{0.53} 
\\ \hline  &  &  & \bf{mean} & \multicolumn{0}{|r|}{\bf{0.58}} & \multicolumn{0}{|r|}{\bf{0.57}} & \multicolumn{0}{|r|}{\bf{0.57}}

\\ \hline Combined & None & SVM & salutation & \multicolumn{0}{|r|}{0.59} & \multicolumn{0}{|r|}{0.52} & \multicolumn{0}{|r|}{0.55} 
\\ \hline  &  &  & undefined & \multicolumn{0}{|r|}{0.54} & \multicolumn{0}{|r|}{0.58} & \multicolumn{0}{|r|}{0.56}  
\\ \hline  &  &  & valediction & \multicolumn{0}{|r|}{0.52} & \multicolumn{0}{|r|}{0.53} & \multicolumn{0}{|r|}{0.52}  
\\ \hline  &  &  & \bf{mean} & \multicolumn{0}{|r|}{\bf{0.55}} & \multicolumn{0}{|r|}{\bf{0.54}} & \multicolumn{0}{|r|}{\bf{0.54}} 

\\ \hline Combined & Tokenised \& & SVM & salutation  & \multicolumn{0}{|r|}{0.57} & \multicolumn{0}{|r|}{0.57} & \multicolumn{0}{|r|}{0.57} 
\\ \hline  &  Stopwords removed &  & undefined & \multicolumn{0}{|r|}{0.50} & \multicolumn{0}{|r|}{0.41} & \multicolumn{0}{|r|}{0.45}  
\\ \hline  &  &  & valediction & \multicolumn{0}{|r|}{0.49} & \multicolumn{0}{|r|}{0.58} & \multicolumn{0}{|r|}{0.53} 
\\ \hline  &  &  & \bf{mean} & \multicolumn{0}{|r|}{\bf{0.52}} & \multicolumn{0}{|r|}{\bf{0.52}} & \multicolumn{0}{|r|}{\bf{0.52}} 
\\ \hline
\end{tabular}
\end{center}
\end{table*}

Table~\ref{tab:chapt7_study2_tab8} provides a summary of the classification results. We note that highest mean precision, recall and F1 score was achieved with the Ubuntu IRC dataset with stop words removed. The lowest mean precision, recall and F1 score were against the combined Enterprise \& Ubuntu IRC dataset with stop words removed. For SVM the Huber loss function with no penalty provided the highest mean scores. 

\subsection{Discussion Lexicography}

Our first research question asked a) what are the high-frequency words used and b) what interesting collocations are present in salutation \& valediction messages. We note that thirty-two of the top fifty words were `stop words', as these words are the most commonly used words in the English language this is unsurprising. Table~\ref{tab:chap7_study2_tab7} illustrates that \textit{thanks, can} and \textit{now} appear fifty times or more. We note that: \textit{Thanks} (a noun or interjection used to express gratitude), \textit{Can} (is an auxiliary verb used with a pronoun (e.g. you)) and \textit{Now} (an adverb used to add a time dimension to an action or statement). Interestingly, neither a variation on the standard greeting or farewell (e.g. hello, goodbye) appeared in the top one hundred words. \textit{Hi} was the 120th most frequent word while \textit{later} was the 292nd most frequent word.

Looking at the collocation graph in Fig. 1 we see \textit{morning} collocates with nine words. The most frequent collocates were \textit{this} (left) and  \textit{for} (right). Interestingly, morning does not collocate with \textit{good}, however it does collocate with a username \textit{mdz} on six occasions. Fig. 2 shows the collocation graph for \textit{thanks}. We can see that \textit{thanks} collocates with twenty-nine distinct words. On twenty-four occasions, \textit{thanks} has no collocates (i.e. used as a single word message), also on thirty-four occasions, \textit{thanks} collocates with four distinct usernames (i.e. \textit{bbh, matt, des, aweir}).

The key takeaway from this work is to demonstrate how complex and variable written discourse is. By adopting a corpus based approach, we can understand how language is used within a specific domain. The results of corpus analysis can be used define features for use as part of a deep learning architecture.

\subsection{Discussion Chat boundary classification}

Our second research question asked, can a machine learning classifier algorithm be trained to identify text as a salutation or valediction from real-time chat messages and if so to what degree of precision. Looking at Table~\ref{tab:chapt7_study2_tab8}, a number of points are apparent. Overall SVM performed best in five of the six experiments, the highest mean precision, recall and F1 score achieved was with the Ubuntu dataset with stop words removed. The Huber loss function with no penalty provided the highest level precision across all experiments. 

Combining the two datasets did not yield a significant improvement classifier performance. By combining the datasets, we added more variance to our training and test data, which degraded classifier performance. Interestingly, when stopped words were removed from each dataset, we observed a slight increase in classifier performance except with the combined dataset. Our intuition suggests that by removing stop words, we removed some of the variance from both datasets. 

Overall, the both classifier algorithms achieved average or slightly better than average performance. We see two contributing factors. Firstly, we know there is variability in the language used within salutation and valediction text. We observed that a number of conversations start with an image, URL or emoji rather than an English word. Secondly, for the undefined label, there is even more variability due to the many ways in which humans can express themselves in chat discourse.

The main benefit of such an experiment is to highlight the idea, that neither the NB nor SVM text classifier algorithms are suited to identify specific types of utterances in chat discourse with a high degree of precision. However, we acknowledge that increasing the level of training and test data would be reasonable for further experiments in this area.



% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%------------------------------------------- CONCLUSIONS --------------------------------------------------
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Conclusion}
The purpose of this study was to examine topic modelling for small text corpora (i.e. Instant message conversations). We found that by segmenting messages into periods of intense (bursts) and non-intense (reflections) communication that these segments, when used in conjunction with a text mining tool can be used to provide a higher number of output terms than modelling the entire corpus of messages at once. Furthermore, we found that the message inter-arrival time can be used to determine both burst and reflection periods.

We also found that the terms output from topic modelling bursts and reflection periods, when aggregated, is easier to understand than the text mined terms from the entire message corpus. Additionally, we saw that all four readability tests, topic modelled terms output from aggregated burst and reflection analysis have a lower readability index compared to terms mined from the entire corpus.

Finally, the relationship between optimal output words and the optimal \# clusters had a strong regression effect. In other words, we can use the optimal terms to predict the required number of topic clusters. This result can have a positive benefit for topic modelling practitioners, as it may reduce the iterative approach needed to find the number of topic clusters that produce the largest distinct number of words.

Both SMEs and micro-teams can use the above result to deliver high-value topic mining outputs from their group chat discourse. Teams can focus initially on a corpus-based approach for a particular channel/space. The advantage of a more extensive corpus approach is that topic modelled outputs can be assessed in context. Where words collations exist, this knowledge can be directly applied to place a higher value on terms generated from topic mining tools.

% Hello & Goodbye conclusion

The purpose of this study was to understand whether a text classification algorithm could be used to identify conversation boundaries using salutation and valediction text within a group chat context. Additionally, we adopted a corpus linguistic approach to identify lexical patterns within group chat conversations.  

We found that both NB and SVM provide a modest level of precision identifying opening and closing remarks within a group chat conversations. Additionally, we found that classifier performance varied little between datasets and that removal of stop words increased classifier performance on each data set. Combining datasets saw a slight decrease in classifier performance.

Furthermore, we found that a corpus-based approach can provide useful insights into the mechanics of opening and closing messages of a group chat conversation with the use of collocations. 

These initial results may be of use to SMEs and researchers in understanding issues in the use of NB and SVM for identification of salutations and valedictions. By adopting a finer grained corpus-based approach, additional features may be developed that can be used to provide models with improved performance. We consider that a multi-feature classification model in the form of a neural network could be used to further our work.

In subsequent work, we shall investigate the idea of whether multiple messages form a salutation and validation cluster. Additionally, we shall evaluate our datasets with two other classification algorithms: Decision trees and the ensemble method random forest using increased training and test data.


% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------

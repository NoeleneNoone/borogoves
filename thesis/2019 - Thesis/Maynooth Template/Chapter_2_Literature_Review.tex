% this file is called up by thesis.tex
% content in this file will be fed into the main document

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\chapter{Literature review} % top level followed by section, subsection
\label{ch:LR}

\begin{textsl}
{\small In this chapter, a comprehensive review of background related literature in the field of crowdsourced testing, Cloud outages, topic modelling and chat message segmentation. Important statistical concepts and methodologies are also discussed.}
\end{textsl}

\vspace*{1cm}

%\adjustmtc

%\minitoc
% ----------------------- contents from here ------------------------


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%---------------------------------------- INTRODUCTION ----------------------------------------------------
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Introduction}
With any body of research it is important to outline and recognise a) the prior related research upon which my research is based and b) the key statistical methods that were used to derive results. This chapter is intended to ground the reader with a background in the areas of: continuous software delivery, bug bounty programs, crowdsourced testing, Cloud outage events and real-time chat discourse. Additional this review chapter will cover key statistical principals that were required to produce both analysis and results upon this research is based (e.g. Distribution fitting, Heavy-tailed analysis, Queuing theory). Finally the review will provide discussion on prior research conducted in the field of software reliability, outage detection and chat discourse segmentation.


\section{Cloud Computing}

The following section provides some background information on two common Cloud services: SaaS and PaaS. We then review high profile Cloud outages that have made the media headlines in recent times. Finally, this section concludes with an in-depth look at relevant studies in the field of repairable systems modelling, queuing theory and Cloud outage studies.

\subsection{Software as a Service}

SaaS is defined as a delivery and licensing model in which software is used on a subscription basis (e.g. monthly, quarterly or yearly) and where applications or services are hosted centrally \cite{Cloudbook2015}. \par

The key benefits for software vendors are the ability for software to be available on a continuous basis (on-demand) and for a single deployment pattern to be used. It is this single deployment pattern that can greatly reduce code validation times in pre-release testing, due to the homogeneous architecture. Central hosting also allows for rapid release of new features and updates through automated delivery processes \cite{datacentre2015}. \par

SaaS is now ubiquitous, while initially adopted by the large software vendors (e.g. Amazon, Microsoft, IBM, Google and Salesforce) many SMEs are now using the cloud as their delivery platform of choice \cite{CRN2015providers}. \par


\subsection{Platform as a Service}
PaaS is defined as a delivery and platform management model. This model allows customers to develop and maintain Cloud-based software and services without the need for building and managing a complex Cloud-based infrastructure.

The main attraction of PaaS is that it allows micro teams and SMEs to rapidly develop and deliver Cloud-based software and services. While focusing on their core products and services micro teams and SMEs are less distracted by having to design, build and service a large complex Cloud-based infrastructure. 

However, one drawback of PaaS is that a micro team or SME may not have a full view of the wider infrastructure. Therefore if an outage event occurs at an infrastructure level (e.g. Network, Loadbalancer) a micro team or SME may be unaware of the problem until the issue is reported by a customer.

Many companies now offer PaaS as their core service. Once seen as the preserve of a large organisation (e.g. Amazon EC2, Google Apps and IBM Bluemix) a number of smaller dedicated companies also offer PaaS (e.g. Dokku, OpenShift and Kubernetes) \cite{Paas2016}.


\subsection{Cloud outages}
A cloud outage is the amount of time that a service is unavailable to the customer. While the benefits of cloud systems are well known, a key disadvantage is that when a cloud environment becomes unavailable, it can take a significant amount of time to diagnose and resolve the problem. During this time the platform can be unavailable for all customers. \par

One of the first cloud outages to make the headlines in recent times was the Amazon outage in April 2011. In summary, the Amazon cloud experienced an outage that lasted 47 hours, the root cause of the issue was a configuration change made as part of a network upgrade. While this issue would be damaging enough for Amazon alone, some consumers of Amazon's cloud platform (Reddit, Foresquare) were also affected. \cite{InfoWorld2015outage} \par

Dropbox experienced two widespread outages during 2013 \cite{Talbot013DBoutage, Etherington2013DBoutage}. The first in January, users were unable to connect to the service. It took Dropbox 15 hours to restore a full service. No official explanation as to the nature of the outage was given. The second occurred in May; again users were unable to connect to the service. This outage lasted a mere 90 minutes. Again no official explanation was provided.

\begin {table}
\begin{flushleft}
\caption {Summary of high profile Cloud outages during 2015 \& 2016}
\label{tab:chap2tab1}
\begin{tabular}{| C{2.6cm} | C{2.8cm} | L{7cm} |} \hline
\textbf{Company} & \textbf{Outage Time} & \textbf{Outage Details} 
\\ \hline Amazon & 10 hours  & Local storms in Australia caused Amazon Web Services to lose power. This resulted in some EC2 instances to fail, which affected both SaaS and PaaS customers. 
\\ \hline  Apple iCloud & 12 hours & A DNS error meant that users were unable to make purchases.
\\ \hline Apple iCloud    & 7 hours & iCloud unavailable / poor performance affected 200 million users.
\\ \hline Microsoft & Several days & Users reported issues accessing their Cloud-based mail services. The defect was identified, and a software fix was applied. This fix proved unsuccessful, after that a secondary fix was developed and applied which was successful.
\\ \hline Twitter & 8 hours  & Users experienced general operational problems after an internal software update was applied to the production system with faulty code. It took Twitter 8 hours to debug and remediate the defective code.
\\  \hline Salesforce & 10 hours  &  European Salesforce users had their services disrupted due to a storage problem in their EU Data Centre. After the storage issue was resolved, users reported performance degradation.
\\ \hline  Starbucks & Unspecified &  Scheduled maintenance resulted in the tilling system going off-line.
\\  \hline Symantec & 24 Hours  &  A portal to allow customers to manage their Cloud security services became unavailable. The exact nature of the outage was undisclosed. Symantec was required to restore and configure a database to bring the system back online.
\\ \hline Verizon    & 40 hours & Scheduled maintenance to improve overall reliability.
\\ \hline  Windows Azure & 2 hours & A network infrastructure outage resulted in loss of service for all central US users.
\\ \hline 
\end{tabular}
\end{flushleft}
\end{table}

While great improvements have been made about redundancy, disaster recovery and ring-fencing of key critical services, the big players in cloud computing are not immune to outages. As of mid-2016, a number of high profile outages were catalogued by the CRN website. \cite{CRN2015outage} \cite{CRN2016outage} Table \ref{tab:chap2tab1} provides a summary. \par

\subsection{Studies related to Cloud outages}
Some studies have been conducted on cloud outages and the time observed to resolve problems in repairable systems. \par

Yuan et al. \cite{yuan2014simple} performed a comprehensive study of distributed system failures. Their study found that almost all failures could be reproduced on reduced node architecture and that performing tests on error handling code could have prevented the majority of failures. They conclude by discussing the efficacy of their own static code checker as a way to check error-handling routines. \par

Hagen et al. \cite{hagen2012efficient} conducted a study into the root cause of the Amazon cloud outage on April 21st, 2011. Their study concluded that a configuration change was made to route traffic from one router to another, while a network upgrade was conducted. The backup router did not have sufficient capacity to handle the required load. They developed a verification technique to detect change conflicts and safety constraints, within a network infrastructure before execution. \par

Li et al. \cite{li2013cloud} conducted a systematic survey of public Cloud outage events. Their findings generated a  framework, which classified outage root causes. Of the 78 outage events surveyed they found that the most common causes of outages included: System issues, i.e. (human error, contention) and power outages being the primary root cause. \par

Sedaghat et al. \cite{sedaghat2015hard} modelled correlated failures caused by both network and power failures. As part of the study, the authors developed a reliability model and an approximation technique for assessing a services reliability in the presence of correlated failures. \par 

Potharaju and Navendu \cite{potharaju2013network} conducted a similar study concerning network outages, with focus on categorising intra and inter-data centre network failures. Two key findings include: Network redundancy is most effective at the inter-datacentre level, and interface errors, hardware failures and unexpected reboots dominate root cause determination. \par

Bodik et al. \cite{bodik2012surviving} analysed the network communication of a large-scale web application. Then proposed a framework that achieves a high fault tolerance with reduced bandwidth usage in outage conditions. \par 

Carcary et al. \cite{carcary2014adoption} conducted a study into Cloud Computing adoption by Irish SMEs. The key findings of the study were as follows: Almost half the 95 SMEs surveyed had not migrated their services to the cloud. Of those SMEs that had migrated they had not assessed their readiness to adopt cloud computing. Finally, the study noted that the main constraints for SMEs adoption of Cloud computing were: Security/compliance concerns, lack of IT skills and data protection concerns. \par

\subsection{Studies related repair time modelling and system reliability}

Synder et al. \cite{snyder2015evaluation} conducted a study on the reliability of Cloud-based systems. The authors developed an algorithm based on a non-sequential Monte Carlo Simulation to evaluate the reliability of large-scale Cloud systems. The authors found that by intelligently allocating the correct types of virtual machine instances, overall Cloud reliability can be maintained with a high degree of precision. \par

Kenney \cite{kenny1993estimating} proposes a model to estimate the arrival of field defects based on the number of software defects found during in-house testing. The model is based on the Weibull distribution which arises from the assumption that field usage of commercial software increases as a power function of time. If we think of Cloud outages as a form of field defect, there is much to consider in this model. For example, the arrival of Cloud outages in the field could be modelled with a power law distribution (e.g. Pareto distribution) as a starting point. \par 

Kleyner and O'Connor \cite{o2011practical} propose an important thesis regarding reliability engineering. While the emphasis is placed on measuring reliability for both mechanical and electrical/electronic systems, the authors do broaden their scope to discuss reliability of computer software. One aspect of interest is their discussion of the lognormal distribution and its application in modelling for system reliability with wear out characteristics and for modelling the repair times of maintained systems. \par

Almog \cite{almog1979study} analysed repair data from twenty maintainable electronic systems to validate whether either the lognormal or exponential distribution would be a suitable candidate distribution to model repair times. His results showed that in 67\% of datasets the lognormal distribution was a suitable fit, while the exponential was unsuitable in 62\% all of datasets. \par

Adedigba \cite{adedigba2005statistical} analysed the service times from a help desk call centre. Her study showed that the exponential distribution did not provide a reasonable fit for call centre service times. However, a log-normal distribution was a reasonable fit for overall service times. Her study also showed that a phase-type distribution with three phases provided a reasonable fit for service times for specific jobs within the call centre job queue. \par

% Added a new reference based on feedback from reviewer #2
Alsoghayer and Djemame \cite {alsoghayer2014resource} propose a mathematical model to predict the risk of failures with Grid and Cloud-based infrastructures. The model uses the observed mean time to failure and the mean to repair for prediction. The authors found the best model to predict the time between failures is a Weibull distribution, while the repair (service) times as best modelled by a lognormal distribution.

\section{Software Development Models}

The software development process typically involves dividing software development work into distinct phases (i.e. Requirement gathering, design, implementation and verification) in order to deliver software to the end user. This framework and methodology began during the late 1960's \cite{elliott2004global}. In this section, we review three methodology's that have been used in recent times: Waterfall, Agile and Continuous Delivery. While our focus is on the latter method, the following section should serve as a potted history of software development models. 

\subsection{Waterfall}

The waterfall development model is a stepped approach to software development. As a development phase is completed, the process moves down (like a waterfall) to the next phase or step. A waterfall method will typically include the following phases: Requirements gathering, design, Implementation, testing, Integration, deployment and maintenance. The process was first mentioned by Winston Royce in 1970 \cite{royce1987managing}. 

Waterfall is seen as an inflexible development model, due mainly to the idea that phases cannot be started until a preceding phase has been completed. This can lead to extended periods of development and test before release \cite{parnas1986rational}. In some cases, a software project could enter an 18 month or longer development duration before market release \cite{mcconnell2004code}. Waterfall is seen as a traditional development model and was used extensively during the 1980's and 1990's. 

\subsection{Agile}

Agile software development is a set of values and methodologies whereby software solutions are developed using an iterative based approach by utilising self-organising cross-functional teams. Agile development was popularised by the publication ``Manifesto for agile software development'' \cite{beck2001manifesto} in 2001. Agile is comprised of two main frameworks: Kanban and Scrum. 

Kanban is a method to visualise workflows within teams. Kanban typically focuses on lanes of work. These lanes are collections of tasks that have either been completed, are in progress or to be completed. By using this visual methodology, teams can graphically represent the progress of an overall project. Kanban can be used to determine overall project effort and productivity at each stage \cite{sugimori1977toyota}.

Scrum proposes that teams are divided into squads or between three to nine developers. These developers break their actions into discrete tasks that can be completed in a fixed timeframe called a ``sprint''. Daily stand-up meetings are used to assess progress with each task. Tasks that cannot be completed within the sprint timeframe can be moved to a backlogged state and is referred to as technical debt \cite{schwaber2002agile}. 

As with multiple frameworks, there are advocates on both sides for using Kanban or Scrum. However, teams are most successful when they adopt elements of both frameworks \cite{kniberg2009kanban}.

In some ways, Agile is seen as the antithesis of waterfall, because a specific feature component can be made available from design, development, test and delivery in a relatively short timeframe (i.e. three week sprint). It should be noted that this iterative approach of design to release is mostly an embodiment of design thinking \cite{lindberg2011design}.

\subsection{Continuous Delivery}
CD is an approach to software development that allows software companies to develop, test and release software, in short, discrete delivery cycles. Releasing software with a low number of changes allows the rapid validation and release of a software product. CD Employs two methodologies; continuous test automation (CA) ---  the practice of employing an automated test script to validate delivered code and continuous integration (CI) --- the practice of merging developer streams into a consolidated mainline, which allows software to be developed and tested to a high standard (due to the low level of code churn), and facilitates a swift release cycle. CD is used as part of a new wave of development, test, deployment and release strategies for Cloud-based software services. Key evangelists for CD include Facebook, Google and Netflix \cite{quora2014}. 

\section{Crowdsourced Testing \& Field Defect Studies}

In this section, we review some themes related to the area of both software testing and studies related to field defect detection. We concentrate our literature review, relative to testing methods (i.e. Crowdsourced testing, Bug Bounties and Dogfood testing) that we believe will enhance and improve the test experience for SMEs and micro teams. It is not an exhaustive review of software testing as a whole. To explore this section in greater detail is beyond the scope of this thesis. Finally, we round off this section with a review of studies related mainly to review of field defects.

\subsection{Crowdsourced testing}

Crowdsourcing is the act of taking a job traditionally performed by a designated agent (usually an employee) and outsourcing it to an undefined, generally large group of people in the form of an open call\cite{crowdsourcedef}.

Nebling et al. \cite{nebeling2012crowdsourced} present a study of Crowdstudy, a toolkit for crowdsourced testing of web pages. By crowdsourcing numerous individuals, data was collected on how users habits differed when engaging with a website. 

Vukovic \cite{Vukovic2009crowdsourcing} conducted a study of crowdsourcing services for the Cloud. While Amazon's Mechanical Turk and Innocentive appear to have the most supported features, most of the frameworks fall short in facilitating the dynamic formation of globally distributed teams. 

Liu et al. \cite{liu2012crowdsourcing} conducted a study into the use of crowdsourcing for usability testing compared to traditional face-to-face methods. Their study found the quality of results from crowdsourcing was not as good as those face-to-face testing. However, crowdsourcing still represents value for design/development teams with limited time and money.

Zogaj et al. \cite{zogaj2014managing} present a case study with a crowdsourcing company who specialise in outsourcing software testing to specific groups. Their research found that there were three key challenges: managing the process, managing the crowd and managing the technology. By using an intermediary to manage all aspects of the process from procurement of individuals, monitoring of test progress to addressing technology skill gaps ensured a smooth end to end process. 

\subsection{Bug Bounty Programs}

A bug bounty program is a scheme whereby software companies offer a reward to users that find defects within their software. The benefit to software companies is that it incentivises users to find defects (typically security vulnerabilities) before they are exploited by the general user base \cite{wiki2015bugbounty}. The \emph{bugcrowd} website contains a list of current bug bounties offered by software companies.  Currently, 116 companies are listed as having some form of reward and gift system for user found vulnerabilities \cite{bugcrowd2015}. Bug bounty schemes are not limited to start-up companies or open source projects.  Some high profile software companies which participate in bug bounty schemes include Facebook, Google and Microsoft. \par

Recently there have been some great bug bounties. For example, Donald Knuth a computer scientist and creator of the TeX computer system \cite{Knuth2015} devised a bug bounty program (Knuth reward checks) where the reward doubled every year in value to a maximum of  \$327.68 in the form of a cashier's check. A second well-known bounty is related to D.J. Bernstein who is a cryptologist and programmer of qmail \cite{Bernstein2015}. In 1997 he offered \$500 to the first individual who could publish details of security exploits within his latest release of qmail. To date, no one has found any vulnerability.

\subsection{Eating your own dogfood}

Eating your own dogfood is a term given to the internal usage of a software product before release to the customer. The idea is that regular internal usage will improve overall software quality. 

Warren Harrison \cite{harrison2006eating} the then editor in chief of IEEE Software, mentions that Microsoft was one of the first companies to aggressively adopt the practice of ``Eating your own dogfood'' when developing their Windows platform in the early 1990's. Harrison also discusses the pros and cons of adopting a dog food approach to internal testing.

Adam Moskowitz \cite{moskowitz2003eat} discussed the idea of dog fooding in the magazine ``;login:''.  In the realm of system administration, Moskowitz provides some practical examples of how increased internal testing can help improve shell scripting and tooling.

Schmidt and Varian \cite{schmidt2005google}, in a Newsweek article, outlined ten rules, which they believe will drive success within Google over the next quarter of a century. They attribute the success of Gmail to the fact that it was extensively tested by the majority of Google employees over a several month period.

Prli\'c and Procter \cite{prlic2012ten} in a Public Library of Science computer biology journal, also outline ten rules from the open development of scientific software. Rule three mentions how software in development should be used as an end product and not merely to demonstrate a solution. In other words, the software should be consumable by customers with a broad range of backgrounds rather than a specific cohort.

Jackson and Winn \cite{jackson2012eating} researched the field of research data management platforms. In building a large complex platform with many API endpoints, they cite internal usage of the in-development platform coupled with the adoption of agile practices such as `Continuous Integration'\cite{wikiCI} as key methods in defect detection.

\subsection{Studies related to defect detection}
Some studies were conducted on customer reported defects. None of the software studied was developed using a CD  release model. \par

Brooks and Robinson \cite{brooks2009initial} performed a study on customer reported GUI defects found on two industrial software systems. Their work focused on the impact, location and resolution times of customer defects. Their study compared these factors from both in-house and customer defects. They found that in-house testers and customers found the same types of defects. 60\% of the total defects found were in the GUI while the remaining 40\% were application defects. Finally, that customers had to wait 170 days for defects to be fixed. \par

Moritz \cite{moritz2009case} conducted a study of customer defects raised within a large industrial telecommunications software component. Her work focused on the analysis of customer defects found over a 10-year period with a goal of understanding how to improve future test quality. She reviewed whether defect regression, test phase, new functionality, load testing and environment were factors in a customer defect being raised. Her study found first, that the in-house system test environments and test use cases did not accurately match customer configurations or usage conditions. Second, that regression testing was inadequate; tests plans typically focused on new features,  which left test exposures within legacy components. Finally, existing test methods were not suitable for finding customer defects. \par

 Gittens et al. \cite{gittens2002empirical} studied the efficiency of in-house software testing by investigating the scope and coverage of system and regression testing. They examined some factors, such as the number of defects found in-house, by the customer and code coverage. Firstly with test coverage in the range of 71 -- 80\% fewer customer defects are found. Secondly, that in-house tests coverage does not always overlap with customer usage areas. Thus there is a gap between in-house and customer product usage. Finally, that greater in-house test coverage does not automatically translate into fewer customer defects found. The authors demonstrated that test coverage needs to be specifically targeted to reduce field defects. \par
 
Musa \cite{musa1996software} developed a technique for Software Reliability Engineered Testing (SRET), which was implemented on the \emph{Fone Follower} project at AT\&T. Musa used a SRET method to classify defects found into four levels of severity based on their impact on the end user. Defect severity rates from prior regression testing were then used to guide future test coverage. \par

Sullivan and Chillarege \cite{sullivan1992comparison} compared the types of customer defects found in Database Systems (DBS) and Operating Systems (OS). Their study looked at some factors including; error type, trigger and defect type. They had some key findings. Firstly they found that legacy DBS and OS had a similar number of high severity defects. Secondly, that newer DBS had a higher rate of high severity defects. \par

Adams \cite{adams1984optimizing} conducted a study of customer defects from nine products over a five-year period. He found that customer defects were typically discovered shortly after the product was released. He surmised that these defects would have taken many person months to find had they been tested on a single machine. He concluded that these customer defects would have been very difficult to find using existing test methods. \par

Riungu et al. \cite{riungu2010research} performed research into the challenges Cloud computing presents to software testing. One concern raised was the human effort to test software with 24/7 availability. Automation aside, they mentioned the need for some level of manual testing to be conducted round the clock---an idea not easily implemented by SMEs.

\section{Data modelling}

Some techniques have been developed over time to model data. In this section, we shall discuss two specific approaches. The first is a parametric technique whereby we assume that our data belongs to a given distribution type. A second approach is a non-parametric approach, whereby there is no underlying assumption about our data. In other words, our data is said to be distribution free. Also discussed in this section are methods to test the goodness of fit of a parametric approach, the background behind heavy-tailed data, and finally a technique to assist in the fitting of count data where the data may be under or overdispersed.

\subsection{Distribution Fitting}

Probability distribution fitting is the fitting of a known probability distribution to a data set regarding the repeated measurement of a variable phenomenon. The type of fitted distribution can vary depending on the underlying data set. The main purpose of distribution fitting is to predict the probability or to forecast the frequency of occurrence of the magnitude of the phenomenon in a certain interval.

There are two main fitting techniques used. The first method is called the method of moments. This method uses expected values of a random variable (a moment) from a population. A sample is then taken from the population and subsequent moment is estimated. The sample moments are used to make estimates about an unknown population. This idea was first proposed by 
Karl Pearson in 1894\cite{pearson1894contributions}.

The second method is called Maximum likelihood estimation (MLE). MLE is a method to estimate the parameter values of a model by determining parameter values that maximise the likelihood.  This technique was first proposed by Ronald Fisher in the 1920's \cite{fisher1925theory}, with a subsequent formal proof by Samuel Wilks in 1938 \cite{wilks1938large}.

\subsection{Goodness of Fit Testing}

If a suitable probability distribution can be found to fit a data set, of interest is how well the distribution fit that data. Some methods have been developed to assess the goodness of fit of a distribution to a data set. We shall discuss three of the main tests briefly.

The Cram\'er--von Mises criterion \cite{cramer1928composition}\cite{von1928statistik} is a non-parametric test which examines the goodness of fit of a cumulative distribution function (CDF) compared to that of an empirical density function (EMF). Using a significance test, we can test a hypothesis of whether a data set is drawn from a given probability distribution

The Kolmogorov--Smirnov \cite{smirnov1948table} test quantifies a distance between the EMF of the sample and the CDF of the reference distribution, or between the EMF of two samples. The idea being that the closer the distance between the two, the better the fit. 

The Anderson--Darling \cite{anderson1954test}\cite{anderson1952asymptotic} test is a statistical test of whether a given sample of data is drawn from a given probability distribution. This test is a modification of the Kolmogorov--Smirnov test as it gives more weight to the tails of data. 

\subsection{Heavy Tailed Estimation}

In probability theory, heavy-tailed distributions are distributions whose tails are not exponentially bounded. In fact, these distributions often have much heavier tails, for example, a Pareto or Generalised extreme value distribution. For such distributions a tail index, which is essentially the shape parameter of a distribution is used to make inferences about the underlying data. 

Hill\cite{hill1975simple} proposes one of the first methods to infer tail behaviour of a distribution function. This work is valuable in that no prior assumption of the type of distribution is required before inference. His tail estimation technique is one of the standard methods for measuring the index of a heavy-tailed distribution.

Pickands\cite{pickands1975statistical} provides a method to make inferences about the tail of a probability distribution function. This technique is applied to all continuous distribution functions. Pickands method is an alternative method to calculate the index of a heavy-tailed distribution.

Nair et al.\cite{nair2013fundamentals} discuss the idea that heavy-tailed data and their corresponding distributions are a more common occurrence. They also discuss various techniques to model distributions from heavy-tailed datasets.

\subsection{Hurdle Distribution}

Hurdle distributions are a class of distributions for count data that can help manage datasets with a large number of zeros or a count dataset which is exhibits either over-dispersion or under-dispersion. Mullahy\cite{mullahy1986specification} proposes the idea of a hurdle model which provides a more natural means to model over or under-dispersed count data. 

\subsection{Kernel Density Estimation}

For datasets which do not fit a known distribution family, a non-parametric approach can be taken. One such approach is Kernel Density Estimation (KDE). In KDE, a range of kernel (weighting) functions are applied to a dataset plotted as a histogram. The kernel functions are divided into various widths (bandwidth). The goal is to choose the most appropriate kernel bandwidth and function shape that best fits the histogram. Both Rosenblatt\cite{rosenblatt1956remarks} and Parzen\cite{parzen1962estimation} are credited with creating KDE in it's current form. Some significant contributions have been made in the field of KDE. These are discussed briefly below. \par

Kernel performance is measured by either the mean integrated squared error (MISE) or the asymptotic mean integrated squared error (AMISE). Epanechnikov \cite{epanechnikov1969non} proposed a parabolically shaped kernel that minimises AMISE and is therefore optimal. Kernel efficiency is now measured in comparison to the Epanechnikov kernel.

Silverman \cite{silverman1986density} proposes an improved method for bandwidth selection. In his study, if a Gaussian basis function is used to approximate univariate data, and if the underlying density is Gaussian, the optimal choice for the bandwidth parameter is the standard deviation of the samples. This method is known and Silverman's rule of thumb or the Gaussian approximation. 

Sheather and Jones\cite{sheather1991reliable} provided an improved method for data-based selection of the bandwidth in KDE. Their paper included a new bias term in their bandwidth estimate, which provides excellent performance for a broad set of cases.

\subsection{Studies related to modelling real-time communication messaging}

Dewes et al. \cite{dewes2003analysis} conducted a study to better understand network traffic dynamics by examining Internet chat systems. While their primary research output was to demonstrate how to separate chat traffic from other Internet traffic, the authors conducted an analysis of the inter-arrival times of chat messages. The author's hypothesis was as follows: Are the inter-arrival times of chat messages consistent with an exponential distribution? The hypothesis was rejected due to lack of evidence. However, they found the inter-arrival times were more consistent with a heavy-tailed distribution.

Lukasik et al. \cite{lukasik2015modeling} modelled time series data of tweets to understand if a reliable prediction model could be derived to predict future tweets. Their research found that by employing a log-Gaussian Cox process, a higher degree of predictive precision could be achieved. The authors also found that mining text from tweet messages can improve inter-arrival time prediction.

Vande Kerckhove et al. \cite{vande2015markov} provided research into the field of inter-arrival times of electronic communication. The authors investigated the level of inter-event dependence between postings and whether a Markovian process would be suitable to model the memory effect observed in inter-arrival online activities. For their study, the authors used social media data from Twitter and Reddit. Their research concluded that by allowing dependence between message wait times provides for more precise modelling than by fitting against a power-law distribution alone.

Markovitch and Krieger \cite{markovitch2000nonparametric} compared the nonparametric estimation of the probability density function of long-tailed distributions from Internet-based traffic against existing parametric methods. The authors found that neither a Pareto nor an exponential model was a suitable fit to their underlying data. Additionally by using both a Parzen--Rosenblatt kernel and a histogram of variable width (a polygram) a more suitable fit was achieved.

Maioroda and Markovitch \cite{maiboroda2004estimation} discuss the nonparametric estimation of a heavy-tailed probability density function by a variable bandwidth kernel estimator. The authors discuss two approaches: A preliminary transformation to provide an information estimation of tail density and a discrepancy method based on the Kolmogorov-Smirnov statistic to evaluate the bandwidth of the kernel estimator. The authors use Internet-based traffic to validate their models.

Wang \cite{viztweettimes} presents a how-to article on visualising the inter-arrival times of tweets. Using the R programming language the author describes the process to collect, visualise and determining if the inter-arrival times can be modelled by a Poisson process.

Burnap et al. \cite{burnap2014tweeting} consider the models to predict information flow size survival using data derived from the popular social networking site Twitter. To model predict flow size and survival rates, zero-truncated negative binomial and Cox regression models were used. This study did not model the distribution of tweet data. However, it is noted that the number of tweets studied and their survival duration were both heavy-tailed. 

\section{Natural Language Processing}

Tokenisation is a process of converting a sequence of characters (e.g. message discourse) into a series of tokens (strings with an assigned meaning) \cite{jurafsky2014speech}. Therefore, before any analysis is conducted on a text corpus, the text is divided into linguistic elements such as words, punctuation, numbers and alpha-numerics \cite{webster1992tokenization}. 

Stop words are words which are filtered out before or after processing of text discourse \cite{leskovec2014mining}. Stop words typically refer to the most common words in a language; there is no consensus or master list of agreed stop words. The website ``ranks.nl'' provides lists of stop words in forty languages \cite{ranknl}. Hans Luhn, one of the pioneers in the field of information retrieval, is credited with creating the concept of stop words \cite{luhn1960key}.

Stemming is a method of collapsing inflected words to their base or root form \cite{lovins1968development}. For example, the words: fishing, fished and fisher, could be reduced to their root fish. The benefit of stemming can be seen as follows: If one is interested in term frequency, it may be easiest to merely count the occurrences of the word fish rather than it's non-stemmed counterparts.

Lemmatisation is the process of grouping together the inflected words, for analysis as a single entity \cite{manning1995introduction}. On the surface this process may look like the opposite of stemming; however, the main difference is that stemming is unaware of the context of the words and thus, cannot differentiate between words that have other meanings depending on context. For example, the word ``worse''  has ``bad'' as its lemma. This link missed by stemming as a dictionary lookup is needed. Whereas, the word ``talk'' is the root of ``talking''. This reference is matched in both stemming and lemmatisation.

\subsection{Corpus Linguistics}

Corpus linguistics is the study of language as expressed in corpora (i.e. collections) of ``actual use'' text. The core idea is that analysis of expression is best conducted within its natural usage. By collecting samples of writing, researchers can understand how individuals converse with each other. One of the most influential studies in this field was conducted by Ku{\v{c}}era and Francis \cite{kuvcera1967computational}. The authors analysed an American English Corpus, that involved analysis techniques from linguistics, psychology and statistics.

\subsection{Topic Modelling Tools}

Latent Semantic Analysis (LSA) is a method that allows for a low-dimension representation of documents and words. By constructing a document-term matrix, and using matrix algebra, one can infer document similarity (product of row vectors) and word similarity (product of column vectors). The idea was first proposed by Landauer et al. in 1998\cite{landauer1998introduction}.

In 1999 Hofman proposed a statistical technique of two-mode and co-occurrence data \cite{hofmann1999probabilistic}. In essence, his Probabilistic Latent Semantic Analysis model (PLSA), allowed a higher degree of precision for information retrieval than standard LSA models. This is due to the introduction of a novel Tempered Expectation Maximisation technique that used a probabilistic method rather than matrices for fitting. However, one drawback of the PLSA method, is that, as the number words and documents increase, so does the level of overfitting.

Latent Dirichlet allocation (LDA) is a generative statistical model that allows topics within a text corpus to be represented as a collection of terms \cite{blei2003latent}. At its core, LDA is a three-level hierarchal Bayesian model, in which each item in an array is modelled as a finite mixture over an underlying set of topics. Blei et al. first proposed the idea in 2003.

\subsection{Linear Regression}

Linear regression is a statistical technique to model the relationship between two or more variables. Typically, one or more explanatory variables (or independent) expressed as X, are used to predict the a response (or dependent) variable expressed as y. Where one independent variable is used, the process is known as simple linear regression. Where more than one independent variable is used the process is known as multiple linear regression.

At a high level, both sets of variables are plotted in the form of a scatter plot, and a least squares line is fitted between the points on the graph. This approach attempts to fit a straight line between the points plotted. If the two sets of variables have a linear relationship, a suitable linear functional can be obtained in the following form:

\begin{equation}
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{\epsilon}_i
\end{equation}

\subsection{Studies Related to Topic Mining of Small Text Corpora}

Jivani conducts a comparative study of eleven stemmer's, to compare their advantages and limitations \cite{jivani2011comparative}. The study found that there is a lot of similarity regarding performance between the various stemming algorithms. Additionally, a rule-based approach may provide the correct output for all cases, as the stems generated may not always be accurate words. For linguistic stemmer's their output is highly dependent on the lexicon used, and words outside of the lexicon are not stemmed correctly.

Naveed et al. \cite{naveed2011searching} investigates the problem of document sparsity in topic mining in the realm of micro-blogs. Their study found that ignoring length normalisation improves retrieval results. By introducing an ``interestingness'' (level of re-tweets) quality measurement also improves retrieval performance. 

The Biterm topic model is explicitly designed for small text corpora such as instant messages and tweet discourse \cite{yan2013biterm}. Conventional topic models such as LDA implicitly capture the document-level word co-occurrence patterns to reveal topics, and thus suffer from the severe data sparsity in short documents. With these problems identified, Yan et al., proposed a topic model that a) explicitly models word co-occurrence patterns and b) uses the aggregated patterns in the whole corpus for learning topics to solve the problem of sparse word co-occurrence patterns at document-level. 

Yin et al. \cite{yin2014dirichlet} discuss the problem of topic modelling short text corpora such as tweets and social media messages. The core challenges are due to sparse, high-dimensional and large volume characteristics. The authors proposed a Gibbs Sampling algorithm for the Dirichlet model (GSDMM). The authors demonstrated that a sparsity model could achieve better performance than either K-means clustering or a Dirichlet Process Mixture Model for Document Clustering with Feature Partition.

Sridhar \cite{sridhar2015unsupervised} presents an unsupervised topic model for short texts using a Gaussian mixture model. His model uses a vector space model that overcomes the issue of word sparsity. The author demonstrates the efficacy of this model compared to LDA using tweet message data.

Topic Modeling of Short Texts: A Pseudo-Document View Zuo et al. \cite{zuo2016topic} propose a probabilistic model called Pseudo-document-based topic model (PTM) for short text topic modelling. PTM introduces the idea of a pseudo-document to implicitly aggregate short texts against data sparsity. By modelling these pseudo-documents rather than short texts, a higher degree of performance is achieved. An additional sparsity enhancement is proposed that removes undesirable correlations between pseudo-documents and latent topics.

Schofield and Mimmo \cite{schofield2016comparing} investigate the effects of stemmers on topic models. Their research concluded that stemming does not help in controlling the size of vocabulary for topic modelling algorithms like LDA, and may reduce the predictive likelihood. The authors suggest that post-stemming may exploit nuances specific to corpora and computationally more efficient due to the smaller list of words for input.

\section{Text Classification}

Text classification is a subset of document classification, whereby text is required to be labelled as a specific class or category. Classes are selected from an established hierarchy of existing classes. For example, text may be classified by subject, author or emotive tone. 

The classification task was traditionally a manual one. However, in recent times due to the advent of large corpora of text data and relatively cheap computing power, the task is mainly conducted using a machine learning algorithm with varying degrees of success \cite{sebastiani2002machine}.  

Today text classification by computers is used to solve many concrete problems such as sentiment detection (i.e. detecting positive or negative film reviews), email sorting (i.e. sort emails sent by family, business colleagues or a spambot).

\section{Machine Learning}

Machine learning is an area of computer science that allows computers to learn the outcome of a task without being explicitly being programmed to do so \cite{samuel1959some}. Machine learning begins by observing data directly and using this knowledge to infer patterns in data and make decisions or predictions on additional examples. The phrase ``Machine Learning'' was first coined by Arthur Samuel in 1959 while working at IBM \cite{provost1998glossary}. 

Machine learning can employ some algorithms to provide output. These algorithms can be categorised as supervised, unsupervised, semi-supervised and reinforcement. 

Supervised learning is probably the most common type of algorithm used today. The core idea that prior labelled data (known as training data) is used to generate a corresponding matching output from another set of data (known as test data). Ideally, the machine learning algorithm can generalise the training data in a meaningful way to determine a classified label from unseen data \cite{mohri2012foundations}. Examples of supervised algorithms include na\"ive Bayes, Support vector machine (SVM), decision trees and random forest.

With unsupervised learning, prior data is neither labelled or classified. In this case, an algorithm is used to cluster data around data that is inferred as being similar. The main aim of unsupervised learning is to provide exploratory data analysis by inferring hidden patterns or groups \cite{hastie2009unsupervised}. Examples of unsupervised algorithms are k-means clustering, Gaussian mixture models and hidden Markov models.

Semi-supervised learning is a hybrid of both algorithms, typically a small amount of labelled data is used to cluster data into a set of known groups. Reinforcement learning is an approach whereby an algorithm interacts with an environment to determine a set of actions to maximise a reward. This method allows of a level of `ideal behaviour' to be inferred \cite{sutton1998reinforcement}. 

In the following subsections, we discuss some algorithms in more detail.

\subsection{Na\"ive Bayes}

A Na\"ive Bayes classifier is a type of machine learning algorithm that uses Bayes theorem. This algorithm makes a strong (na\"ive) assumption of independence between each pair of features \cite{russell2003artificial}. Despite the seemingly over-simplified assumptions of independence, na\"ive Bayes have shown to be useful with solving real word problems most notably in the field of document classification and email spam filtering \cite{sahami1998bayesian}. Bayes theorem can be defined as follows:

\begin{equation}
P(A \mid B) = \frac{P(B \mid A) \, P(A)}{P(B)}
\end{equation}

\subsection{Decision Trees}

In machine learning a decision tree (DT) is a non-parametric supervised learning algorithm that uses observed data to make decisions about unseen data \cite{quinlan1986induction}. As the name implies, labelled data is represented in the form of a tree. Decisions are made by the underlying data in the form of a branch (feature intersections) and leaf-like (class labels) structure. Some advantages of DT's over other types of classifiers include: As DT's (and their results) is visualised graphically, therefore they are easy to interpret. Furthermore, DT's use a white box model, for any given observation, the explanation for its result and outcome as easily explained. DT's do have problems in that the models created do not generalise as well as other models.

\subsection{Support Vector Machine}

Support vector machine (SVM) is an algorithm used in machine learning to solve classification and regression problems \cite{cortes1995support}. SVM represents labelled observations as points in space. As the points as plotted the algorithm determines what line best separates the labelled classes. This separation point is also known as a hyperplane. Ideally, a hyperplane with the largest distance between both sets of classes is preferable, as this makes it easier to distinguish between classes.

If a classification problem presents whereby a line is unable to separate the labelled classes successfully, SVM can use a non-linear classification. A kernel trick is used to perform a data transformation to create a high dimension feature space. As a result the hyperplane may be extended to a curve or a series of curves. The kernel trick was initially proposed as far back as 1964 by Mark Aizerman \cite{aizerman1964theoretical}. Vapnik et al. are credited with successfully incorporating the kernel trick to SVM in the early 1990's \cite{boser1992training}. 

Due to SVM's flexibility, the algorithm has been used to solve many real world problems in the field of text and image classification (face recognition) \cite{osuna1997training}. Additionally in the field of bioinformatics, SVM is been shown to be an effective method to classify proteins \cite{liao2003combining}. 

\subsection{Random Forest}

Random forest is an ensemble algorithm classifier that is used in machine learning. Rather than being a distinct classifier in of itself, it is a collection of techniques used for classification \cite{ho1995random}. Random forests work by building a number of decision trees and outputs a class that is the most prevalent. Random forests are used to correct the behaviour of decision trees, that overfit to their training data \cite{friedman2001elements}. An additional ensemble method includes a technique known as bagging. Bagging involves a random selection of features at the training stage \cite{breiman1996bagging}. This random selection is used to reduce the level of variance in an estimator.



%\subsection{Neural Networks}
%** Remains to be seen whether we build a NN given the limited level of training data.

\subsection{Studies Related to Text Segmentation}

The purpose of text segmentation is to identify specific regions of text within a corpus. The benefit of such a practice is to aid in the field of information retrieval, where topic boundary identification is a crucial problem. We discuss some of the leading contributions to the domain of topic boundary identification briefly.

One of the first studies (1991) in the field of text segmentation was conducted by Morris and Hirst \cite{morris1991lexical}. The authors focused on the problem of lexical cohesion (chains of related words), by using a thesaurus as a knowledge base for computing lexical chains. Additional early contributors in the field of lexical cohesion include Kozima \cite{kozima1993text}, who proposed a lexical cohesion profile, and Reynar \cite{reynar1994automatic} who outlined an improved method of locating discourse boundaries based on the previous method of lexical cohesion and a graphical technique call dotplotting. 

Some years later, additional techniques have been used to tackle the problem of partitioning text into coherent segments. Beeferman et al. \cite{beeferman1999statistical} introduce an exponential model to extract features that are correlated to the presence of boundaries. Their study used \textit{Wall Street Journal} news articles and television news story transcripts.  Galley et al. \cite{galley2003discourse} propose a discourse segmentation technique using for multi-party conversations. Their lexical cohesion algorithm demonstrated reasonable results when text extracted from the Brown corpus \footnote{http://clu.uni.no/icame/brown/bcm.html}. 

In more recent times (2012 onwards), new researchers used different techniques to research text segmentation. Nguyen et al. \cite{nguyen2012sits} proposed a Bayesian nonparametric model to discover the topics used in a conversation, topic shift and a person specific tendency to introduce new topics. The authors used transcripts from the 2008 presidential debate and a television programme called Crossfile. Brooks et al. \cite{brooks2013statistical} used a machine learning approach to identify effective state (e.g. joy excitement, confusion, frustration, anger and annoyance) on chat logs, using comprised of discussion from an astrophysics institute. Schmidt and Stone \cite{schmidt2013detection} use a combination of techniques (i.e. Latent semantic analysis, text tiling, and pause detection) to detect topic changes in IRC chat logs, with limited success.  

Rounding off our studies in this section, Uthus and Aha \cite{uthus2013multiparticipant} surveyed research on the analysis of multi-participant chat. The authors conclude that chat data is difficult to analyse due to its unique characteristics due to the many problems the medium presents (e.g. Chat room feature processing, thread disentanglement, topic detection, summarisation and user profiling). This has caused many traditional text analysis techniques to prove unsuccessful. The authors suggest that given its prevalence of social communication, the domain represents an exciting research topic. 

\section{Conclusion}

It may be argued, with the seemingly endless amount of data generated today, the challenge to make sense of this data is insurmountable. It is fair to acknowledge that data generation of many kinds is on the increase. Nevertheless, while this work addresses three specific domains in computer science, we demonstrate that with the right analysis toolkit, converting data to information is possible.

Field defects are a useful metric to understand the quality of a given product or service. The orthodoxy states that software should be as bug-free as possible before releasing to market. While this was true of software delivered using protracted development cycles (Waterfall), this may hold true. Through analysis of defect data, it is essential to understand the role the customer plays in field defect discovery. Chapter 3 address this requirement. 

Cloud outages are seen as the most severe type of defect, given the central point of failure. In other words, if a Cloud infrastructure is unavailable, a software vendor may lose vital revenue from downtime. Therefore it is crucial that analysis is conducted on both the time between outage events and the time to service such events. The work involved these two case studies became chapter 4.

Modelling the inter-arrival and service times of Cloud outages appeared a useful exercise in its own right. However, for small teams, being able to schedule team planning around future Cloud outages became a logical conclusion of this work. Chapter 5 looks at the modelling result of outage inter-arrival and service times and combining with a simple queue model to predict outage busy times. 

From research into Cloud outage events, it became clear that DevOps teams, use real-time chat applications to discuss, diagnose and resolve outage events. We believed that the duration of these discussions would provide a useful analogue to Cloud outage service times. With this in mind Chapter 6 turned our attending to modelling real-time chat discourse conversations.

Finally, through the modelling research conducted in chapter 6, we extended our chat discourse analysis first to investigate techniques to enhance topic modelling to improved understanding and readability of such outputs. Additionally, we wanted to close this body of work by looking at chat boundaries. Was it possible to train a computer to detect boundaries within conversations? Chapter 7 comprises of this work. 









Then use a paragraph to frame chapter 3,4,5,6,7 work then a final summing up.




% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%------------------------------------------- CONCLUSIONS --------------------------------------------------
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %



% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
